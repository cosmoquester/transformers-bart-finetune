{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih731fUio05R"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/cosmoquester/transformers-tf-finetune.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrvYJ8reqBFV"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "import urllib.request\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "\n",
        "from transformers_tf_finetune.losses import PearsonCorrelationLoss\n",
        "from transformers_tf_finetune.metrics import (\n",
        "    BinaryF1Score,\n",
        "    PearsonCorrelationMetric,\n",
        "    SpearmanCorrelationMetric,\n",
        "    pearson_correlation_coefficient,\n",
        "    spearman_correlation_coefficient,\n",
        ")\n",
        "from transformers_tf_finetune.models import SemanticTextualSimailarityWrapper\n",
        "from transformers_tf_finetune.utils import LRScheduler, get_device_strategy, path_join, set_random_seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjkDo06wuRAW"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S1L2-t7qRxD"
      },
      "outputs": [],
      "source": [
        "#: transformers pretrained path\n",
        "pretrained_model = \"cosmoquester/bart-ko-small\"\n",
        "#: pretrained tokenizer fast pretrained path\n",
        "pretrained_tokenizer = \"cosmoquester/bart-ko-small\"\n",
        "#: load from pytorch weight\n",
        "from_pytorch = False\n",
        "#: use huggingface credential for private model\n",
        "use_auth_token = \"\"\n",
        "\n",
        "train_dataset_path = \"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorSTS/sts-train.tsv\"\n",
        "dev_dataset_path = \"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorSTS/sts-dev.tsv\"\n",
        "test_dataset_path = \"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorSTS/sts-test.tsv\"\n",
        "#: output directory to save log and model checkpoints, should be GCS path with TPU\n",
        "output_path = None\n",
        "\n",
        "#: training params\n",
        "epochs = 5\n",
        "learning_rate = 5e-5\n",
        "min_learning_rate = 1e-5\n",
        "warmup_rate = 0.06\n",
        "warmup_steps = None\n",
        "batch_size = 128\n",
        "dev_batch_size = 512\n",
        "tensorboard_update_freq = 1\n",
        "\n",
        "#: device to use (TPU or GPU or CPU)\n",
        "device = \"TPU\"\n",
        "#: Use mixed precision FP16\n",
        "mixed_precision = False\n",
        "#: Set random seed\n",
        "seed = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqP3q8KZvTXg"
      },
      "outputs": [],
      "source": [
        "if output_path is not None and output_path.startswith(\"gs://\"):\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ6nno7nrgQH"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_path: str, tokenizer: AutoTokenizer, shuffle: bool = False) -> tf.data.Dataset:\n",
        "    \"\"\"\n",
        "    Load KorSTS dataset from local file or web\n",
        "\n",
        "    :param dataset_path: local file path or file uri\n",
        "    :param tokenizer: PreTrainedTokenizer for tokenizing\n",
        "    :param shuffle: whether shuffling lines or not\n",
        "    :returns: KorSTS dataset, number of dataset\n",
        "    \"\"\"\n",
        "    if dataset_path.startswith(\"https://\"):\n",
        "        with urllib.request.urlopen(dataset_path) as response:\n",
        "            data = response.read().decode(\"utf-8\")\n",
        "    else:\n",
        "        with open(dataset_path) as f:\n",
        "            data = f.read()\n",
        "    lines = data.splitlines()[1:]\n",
        "    if shuffle:\n",
        "        random.shuffle(lines)\n",
        "\n",
        "    start_token = tokenizer.bos_token or tokenizer.cls_token or \"\"\n",
        "    end_token = tokenizer.eos_token or tokenizer.sep_token or \"\"\n",
        "\n",
        "    sentences1 = []\n",
        "    sentences2 = []\n",
        "    normalized_labels = []\n",
        "    for *_, score, sentence1, sentence2 in csv.reader(lines, delimiter=\"\\t\", quoting=csv.QUOTE_NONE):\n",
        "        sentences1.append(start_token + sentence1 + end_token)\n",
        "        sentences2.append(start_token + sentence2 + end_token)\n",
        "        normalized_labels.append(float(score) / 5.0)\n",
        "\n",
        "    inputs1 = dict(\n",
        "        tokenizer(\n",
        "            sentences1, padding=True, return_tensors=\"tf\", return_token_type_ids=False, return_attention_mask=True,\n",
        "        )\n",
        "    )\n",
        "    inputs2 = dict(\n",
        "        tokenizer(\n",
        "            sentences2, padding=True, return_tensors=\"tf\", return_token_type_ids=False, return_attention_mask=True,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(((inputs1, inputs2), normalized_labels))\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTej5sm8r5Jj"
      },
      "outputs": [],
      "source": [
        "if seed:\n",
        "    set_random_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka-cxaFGsMqr"
      },
      "outputs": [],
      "source": [
        "strategy = get_device_strategy(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4xdNRu2tiZq"
      },
      "source": [
        "# Mixed Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0UhTl2BtIIQ"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    if mixed_precision:\n",
        "        mixed_type = \"mixed_bfloat16\" if device == \"TPU\" else \"mixed_float16\"\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy(mixed_type)\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWVZH3-VuWHh"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a73NxJD5tk8s"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer, use_auth_token=use_auth_token)\n",
        "\n",
        "    train_dataset = load_dataset(train_dataset_path, tokenizer, True)\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    dev_dataset = load_dataset(dev_dataset_path, tokenizer).batch(dev_batch_size)\n",
        "    test_dataset = load_dataset(test_dataset_path, tokenizer).batch(dev_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYL-lnWOuZgm"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLPZGWhdttFm"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model = TFAutoModel.from_pretrained(\n",
        "        pretrained_model, use_auth_token=use_auth_token, from_pt=from_pytorch\n",
        "    )\n",
        "    model_sts = SemanticTextualSimailarityWrapper(model=model, embedding_dropout=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s_B01rauc7S"
      },
      "source": [
        "# Model Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdIgKdMVtw0q"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model_sts.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "            LRScheduler(\n",
        "                len(train_dataset) * epochs,\n",
        "                learning_rate,\n",
        "                min_learning_rate,\n",
        "                warmup_rate,\n",
        "                warmup_steps,\n",
        "            ),\n",
        "        ),\n",
        "        loss=[PearsonCorrelationLoss(), tf.keras.losses.MeanSquaredError()],\n",
        "        loss_weights=[0.25, 0.75],\n",
        "        metrics=[\n",
        "            BinaryF1Score(),\n",
        "            PearsonCorrelationMetric(name=\"pearson_coef\"),\n",
        "            SpearmanCorrelationMetric(name=\"spearman_coef\"),\n",
        "        ],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp5JeBv-ueUN"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a34uOyuQtz71"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model_sts.fit(\n",
        "        train_dataset,\n",
        "        validation_data=dev_dataset,\n",
        "        epochs=epochs,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                path_join(output_path, \"best_model.ckpt\"),\n",
        "                save_weights_only=True,\n",
        "                save_best_only=True,\n",
        "                monitor=\"val_spearman_coef\",\n",
        "                mode=\"max\",\n",
        "                verbose=1,\n",
        "            ),\n",
        "            tf.keras.callbacks.TensorBoard(\n",
        "                path_join(output_path, \"logs\"), update_freq=tensorboard_update_freq\n",
        "            ),\n",
        "        ] if output_path is not None else None,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGFUZNarugBN"
      },
      "source": [
        "# Model Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64m9DkPzt7XJ"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    preds = []\n",
        "    labels = []\n",
        "    f1 = BinaryF1Score()\n",
        "    for inputs, label in test_dataset:\n",
        "        pred = model_sts(inputs, training=False)\n",
        "        preds.extend(pred.numpy())\n",
        "        labels.extend(label.numpy())\n",
        "        f1.update_state(label, pred)\n",
        "\n",
        "    pearson_score = pearson_correlation_coefficient(labels, preds)\n",
        "    spearman_score = spearman_correlation_coefficient(labels, preds)\n",
        "    print(\n",
        "        f\"Dev F1 Score: {f1.result():.4f}, \"\n",
        "        f\"Dev Pearson: {pearson_score:.4f}, \"\n",
        "        f\"Dev Spearman: {spearman_score:.4f}\"\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_korsts.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
