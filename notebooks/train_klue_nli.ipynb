{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "train-klue-nli",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install git+https://github.com/cosmoquester/transformers-tf-finetune.git"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ih731fUio05R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\n",
        "import random\n",
        "import urllib.request\n",
        "from typing import Dict\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import AdamWeightDecay, AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "from transformers_tf_finetune.utils import (\n",
        "    LRScheduler,\n",
        "    get_device_strategy,\n",
        "    path_join,\n",
        "    set_random_seed,\n",
        "    tfbart_sequence_classifier_to_transformers,\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "vrvYJ8reqBFV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "tfbart_sequence_classifier_to_transformers()"
      ],
      "outputs": [],
      "metadata": {
        "id": "kwmp9q9AqOR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "xjkDo06wuRAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#: transformers pretrained path\n",
        "pretrained_model = \"cosmoquester/bart-ko-small\"\n",
        "#: pretrained tokenizer fast pretrained path\n",
        "pretrained_tokenizer = \"cosmoquester/bart-ko-small\"\n",
        "#: load from pytorch weight\n",
        "from_pytorch = False\n",
        "#: use huggingface credential for private model\n",
        "use_auth_token = \"\"\n",
        "\n",
        "train_dataset_path = \"https://raw.githubusercontent.com/KLUE-benchmark/KLUE/main/klue_benchmark/klue-nli-v1.1/klue-nli-v1.1_train.json\"\n",
        "dev_dataset_path = \"https://raw.githubusercontent.com/KLUE-benchmark/KLUE/main/klue_benchmark/klue-nli-v1.1/klue-nli-v1.1_dev.json\"\n",
        "#: output directory to save log and model checkpoints, should be GCS path with TPU\n",
        "output_path = None\n",
        "\n",
        "#: training params\n",
        "epochs = 10\n",
        "learning_rate = 5e-5\n",
        "min_learning_rate = 1e-5\n",
        "warmup_rate = 0.06\n",
        "warmup_steps = None\n",
        "batch_size = 128\n",
        "dev_batch_size = 512\n",
        "num_valid_dataset = 3000\n",
        "tensorboard_update_freq = 1\n",
        "\n",
        "#: device to use (TPU or GPU or CPU)\n",
        "device = \"TPU\"\n",
        "#: Use mixed precision FP16\n",
        "mixed_precision = False\n",
        "#: Set random seed\n",
        "seed = None"
      ],
      "outputs": [],
      "metadata": {
        "id": "1S1L2-t7qRxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if output_path is not None and output_path.startswith(\"gs://\"):\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ],
      "outputs": [],
      "metadata": {
        "id": "wqP3q8KZvTXg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def load_dataset(\n",
        "    dataset_path: str, tokenizer: AutoTokenizer, label2id: Dict[str, int], shuffle: bool = False\n",
        ") -> tf.data.Dataset:\n",
        "    \"\"\"\n",
        "    Load KLUE NLI dataset from local file or web\n",
        "\n",
        "    :param dataset_path: local file path or file uri\n",
        "    :param tokenizer: PreTrainedTokenizer for tokenizing\n",
        "    :param label2id: dictionary for mapping label to index\n",
        "    :param shuffle: whether shuffling lines or not\n",
        "    :returns: KLUE NLI dataset, number of dataset\n",
        "    \"\"\"\n",
        "    if dataset_path.startswith(\"https://\"):\n",
        "        with urllib.request.urlopen(dataset_path) as response:\n",
        "            data = response.read().decode(\"utf-8\")\n",
        "    else:\n",
        "        with open(dataset_path) as f:\n",
        "            data = f.read()\n",
        "    examples = json.loads(data)\n",
        "    if shuffle:\n",
        "        random.shuffle(examples)\n",
        "\n",
        "    start_token = tokenizer.bos_token or tokenizer.cls_token\n",
        "    end_token = tokenizer.eos_token or tokenizer.sep_token\n",
        "    sep = tokenizer.sep_token\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    for example in examples:\n",
        "        sentences.append(start_token + example[\"premise\"] + sep + example[\"hypothesis\"] + end_token)\n",
        "        labels.append(label2id[example[\"gold_label\"]])\n",
        "\n",
        "    inputs = dict(\n",
        "        tokenizer(\n",
        "            sentences,\n",
        "            padding=True,\n",
        "            return_tensors=\"tf\",\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "    return dataset"
      ],
      "outputs": [],
      "metadata": {
        "id": "NZ6nno7nrgQH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if seed:\n",
        "    set_random_seed(seed)"
      ],
      "outputs": [],
      "metadata": {
        "id": "BTej5sm8r5Jj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "strategy = get_device_strategy(device)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ka-cxaFGsMqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mixed Precision"
      ],
      "metadata": {
        "id": "p4xdNRu2tiZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "    if mixed_precision:\n",
        "        mixed_type = \"mixed_bfloat16\" if device == \"TPU\" else \"mixed_float16\"\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy(mixed_type)\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)"
      ],
      "outputs": [],
      "metadata": {
        "id": "A0UhTl2BtIIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "VWVZH3-VuWHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer, use_auth_token=use_auth_token)\n",
        "\n",
        "    label2id = {\"neutral\": 0, \"entailment\": 1, \"contradiction\": 2}\n",
        "    dataset = load_dataset(train_dataset_path, tokenizer, label2id, True)\n",
        "    train_dataset = dataset.skip(num_valid_dataset).batch(batch_size)\n",
        "    valid_dataset = dataset.take(num_valid_dataset).batch(dev_batch_size)\n",
        "    dev_dataset = load_dataset(dev_dataset_path, tokenizer, label2id).batch(dev_batch_size)"
      ],
      "outputs": [],
      "metadata": {
        "id": "a73NxJD5tk8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "dYL-lnWOuZgm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        pretrained_model,\n",
        "        num_labels=len(label2id),\n",
        "        use_auth_token=use_auth_token,\n",
        "        from_pt=from_pytorch,\n",
        "    )\n",
        "    model.config.id2label = {v: k for k, v in label2id.items()}\n",
        "    model.config.label2id = label2id"
      ],
      "outputs": [],
      "metadata": {
        "id": "LLPZGWhdttFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Compile"
      ],
      "metadata": {
        "id": "7s_B01rauc7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "    outputs = model(tf.keras.Input([None], dtype=tf.int32), return_dict=True)\n",
        "    training_model = tf.keras.Model({\"input_ids\": model.input}, outputs.logits)\n",
        "    training_model.compile(\n",
        "        optimizer=AdamWeightDecay(\n",
        "            LRScheduler(\n",
        "                len(train_dataset) * epochs,\n",
        "                learning_rate,\n",
        "                min_learning_rate,\n",
        "                warmup_rate,\n",
        "                warmup_steps,\n",
        "            ),\n",
        "            weight_decay_rate=0.01,\n",
        "            exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"],\n",
        "        ),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
        "    )"
      ],
      "outputs": [],
      "metadata": {
        "id": "cdIgKdMVtw0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "Kp5JeBv-ueUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "    training_model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=valid_dataset,\n",
        "        epochs=epochs,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                path_join(output_path, \"best_model.ckpt\"),\n",
        "                save_weights_only=True,\n",
        "                save_best_only=True,\n",
        "                monitor=\"val_accuracy\",\n",
        "                mode=\"max\",\n",
        "                verbose=1,\n",
        "            ),\n",
        "            tf.keras.callbacks.TensorBoard(\n",
        "                path_join(output_path, \"logs\"), update_freq=tensorboard_update_freq\n",
        "            ),\n",
        "        ] if output_path is not None else None,\n",
        "    )"
      ],
      "outputs": [],
      "metadata": {
        "id": "a34uOyuQtz71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluate"
      ],
      "metadata": {
        "id": "DGFUZNarugBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with strategy.scope():\n",
        "    loss, accuracy = training_model.evaluate(dev_dataset)"
      ],
      "outputs": [],
      "metadata": {
        "id": "64m9DkPzt7XJ"
      }
    }
  ]
}